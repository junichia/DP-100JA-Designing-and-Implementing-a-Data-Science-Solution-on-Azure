{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# バッチ推論サービスを作成する\n",
        "\n",
        "健康クリニックは一日中患者の測定を取り、各患者の詳細を別々のファイルに保存すると想像してください。その後、一晩で糖尿病予測モデルを使用して、その日のすべての患者データをバッチとして処理し、翌朝待つ予測を生成し、糖尿病のリスクがあると予測される患者をフォローアップできるようにします。Azure Machine Learning では、*バッチ推論パイプライン*を作成することでこれを実現できます。そして、この演習ではそれを実施します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ワークスペースに接続する\n",
        "\n",
        "作業を開始するには、ワークスペースに接続します。\n",
        "\n",
        "> **注**: Azure サブスクリプションでまだ認証済みのセッションを確立していない場合は、リンクをクリックして認証コードを入力し、Azure にサインインして認証するよう指示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# 保存された構成ファイルからワークスペースを読み込む\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## モデルをトレーニングして登録する\n",
        "\n",
        "それでは、バッチ推論パイプラインにデプロイするモデルをトレーニングして登録してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.core import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# ワークスペースで Azure 実験を作成する\n",
        "experiment = Experiment(workspace=ws, name='mslearn-train-diabetes')\n",
        "run = experiment.start_logging()\n",
        "print(\"Starting experiment:\", experiment.name)\n",
        "\n",
        "# 糖尿病データセットを読み込む\n",
        "print(\"Loading Data...\")\n",
        "diabetes = pd.read_csv('data/diabetes.csv')\n",
        "\n",
        "# 特徴とラベルを分離する\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# データをトレーニング セットとテスト セットに分割する\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# デシジョン ツリー モデルをトレーニングする\n",
        "print('Training a decision tree model')\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# 精度を計算する\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# AUC を計算する\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "# トレーニング済みモデルを保存する\n",
        "model_file = 'diabetes_model.pkl'\n",
        "joblib.dump(value=model, filename=model_file)\n",
        "run.upload_file(name = 'outputs/' + model_file, path_or_stream = './' + model_file)\n",
        "\n",
        "# 実行を完了する\n",
        "run.complete()\n",
        "\n",
        "# モデルを登録する\n",
        "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
        "                   tags={'Training context':'Inline Training'},\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "\n",
        "print('Model trained and registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## バッチ データを生成およびアップロードする\n",
        "\n",
        "この演習用の新しいデータを取得するために、スタッフが揃って患者がいる診療所は実在しないため、糖尿病の CSV ファイルからランダムなサンプルを生成し、そのデータを Azure Machine Learning ワークスペースのデータストアにアップロードし、そのためのデータセットを登録します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 既定のデータ ストアを設定する\n",
        "ws.set_default_datastore('workspaceblobstore')\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "# すべてのデータストアを列挙し、どちらが既定かを示す\n",
        "for ds_name in ws.datastores:\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\n",
        "\n",
        "# 糖尿病データを読み込む\n",
        "diabetes = pd.read_csv('data/diabetes2.csv')\n",
        "# 特徴列の 100 項目のサンプルを取得する (糖尿病ラベルではない)\n",
        "sample = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].sample(n=100).values\n",
        "\n",
        "# フォルダーを作成する\n",
        "batch_folder = './batch-data'\n",
        "os.makedirs(batch_folder, exist_ok=True)\n",
        "print(\"Folder created!\")\n",
        "\n",
        "# 各サンプルを個別のファイルとして保存する\n",
        "print(\"Saving files...\")\n",
        "for i in range(100):\n",
        "    fname = str(i+1) + '.csv'\n",
        "    sample[i].tofile(os.path.join(batch_folder, fname), sep=\",\")\n",
        "print(\"files saved!\")\n",
        "\n",
        "# 既定のデータストアにファイルをアップロードする\n",
        "print(\"Uploading files to datastore...\")\n",
        "default_ds = ws.get_default_datastore()\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\n",
        "\n",
        "# 入力データ用データセットを登録する\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\n",
        "try:\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \n",
        "                                             name='batch-data',\n",
        "                                             description='batch data',\n",
        "                                             create_new_version=True)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## コンピューティングを作成する\n",
        "\n",
        "パイプラインのコンピューティング コンテキストが必要なので、次のコードを使用して Azure Machine Learning コンピューティング クラスター (存在しない場合は作成されます) を指定します。\n",
        "\n",
        "> **重要**: 実行する前に、以下のコードで *your-compute-cluster* をコンピューティング クラスターの名前に変更してください。クラスター名は、長さが 2 〜 16 文字のグローバルに一意の名前である必要があります。英字、数字、- の文字が有効です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"your-compute-cluster\"\n",
        "\n",
        "try:\n",
        "    # 既存のコンピューティング先を確認する\n",
        "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # まだ存在しない場合は、作成します\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
        "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "        inference_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## バッチ推論用パイプラインを作成する\n",
        "\n",
        "これで、バッチ推論に使用するパイプラインを定義する準備が整いました。パイプラインではバッチ推論を実行するために Python コードが必要となるので、パイプラインで使用されるすべてのファイルを保存できるフォルダーを作成しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# 実験ファイル用フォルダーを作成する\n",
        "experiment_folder = 'batch_pipeline'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print(experiment_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に、実際の作業を行う Python スクリプトを作成し、パイプライン フォルダーに保存します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $experiment_folder/batch_diabetes.py\n",
        "import os\n",
        "import numpy as np\n",
        "from azureml.core import Model\n",
        "import joblib\n",
        "\n",
        "\n",
        "def init():\n",
        "    # パイプライン手順が初期化されたときに実行する\n",
        "    global model\n",
        "\n",
        "    # モデルを読み込む\n",
        "    model_path = Model.get_model_path('diabetes_model')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "\n",
        "def run(mini_batch):\n",
        "    # これはバッチごとに実行する\n",
        "    resultList = []\n",
        "\n",
        "    # バッチ内の各ファイルを処理する\n",
        "    for f in mini_batch:\n",
        "        # カンマ区切りデータを配列に読み込む\n",
        "        data = np.genfromtxt(f, delimiter=',')\n",
        "        # 予測のために 2 次元配列に変形する (モデルは複数の項目を必要とします)\n",
        "        prediction = model.predict(data.reshape(1, -1))\n",
        "        # 結果に予測をアペンドする\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
        "    return resultList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に、スクリプトで必要な依存関係を含む実行コンテキストを定義します"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "from azureml.core.runconfig import CondaDependencies\n",
        "\n",
        "# モデルに必要な依存関係を追加する\n",
        "# Scikit-learn モデルの場合、Scikit-learn が必要です\n",
        "# 並列パイプラインの手順では、azureml コアと azureml-dataprep[fuse] が必要です\n",
        "cd = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n",
        "                              pip_packages=['azureml-defaults','azureml-core','azureml-dataprep[fuse]'])\n",
        "\n",
        "batch_env = Environment(name='batch_environment')\n",
        "batch_env.python.conda_dependencies = cd\n",
        "batch_env.docker.enabled = True\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "print('Configuration ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "パイプラインを使用して、バッチ予測スクリプトを実行し、入力データから予測を生成し、結果をテキスト ファイルとして出力フォルダーに保存します。これを行うには、**ParallelRunStep** を使用して、バッチ データを並列処理し、結果を *parallel_run_step.txt* という名前の単一の出力ファイルに照合することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
        "from azureml.pipeline.core import PipelineData\n",
        "\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "output_dir = PipelineData(name='inferences', \n",
        "                          datastore=default_ds, \n",
        "                          output_path_on_compute='diabetes/results')\n",
        "\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    source_directory=experiment_folder,\n",
        "    entry_script=\"batch_diabetes.py\",\n",
        "    mini_batch_size=\"5\",\n",
        "    error_threshold=10,\n",
        "    output_action=\"append_row\",\n",
        "    environment=batch_env,\n",
        "    compute_target=inference_cluster,\n",
        "    node_count=2)\n",
        "\n",
        "parallelrun_step = ParallelRunStep(\n",
        "    name='batch-score-diabetes',\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\n",
        "    output=output_dir,\n",
        "    arguments=[],\n",
        "    allow_reuse=True\n",
        ")\n",
        "\n",
        "print('Steps defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次は、ステップをパイプラインに入れて実行します。\n",
        "\n",
        "> **注**: これには時間がかかる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
        "pipeline_run = Experiment(ws, 'mslearn-diabetes-batch').submit(pipeline)\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "パイプラインの実行が終了すると、結果の予測は、パイプラインの最初の (そして唯一の) ステップに関連付けられた実験の出力に保存されます。次のように取得できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# 以前の実行で残っている場合は、ローカル結果フォルダーを削除する\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
        "\n",
        "# 最初のステップの実行を取得し、その出力をダウンロードする\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='diabetes-results')\n",
        "\n",
        "# フォルダー階層を走査して、結果ファイルを見つける\n",
        "for root, dirs, files in os.walk('diabetes-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# クリーンアップ出力形式\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "\n",
        "# 最初の 20 件の結果を表示する\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## パイプラインを公開し、その REST インターフェイスを使用する\n",
        "\n",
        "バッチ推論用の作業パイプラインができたので、それを公開し、REST エンドポイントを使用してアプリケーションから実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "    name='diabetes-batch-pipeline', description='Batch scoring of diabetes data', version='1.0')\n",
        "\n",
        "published_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "公開されたパイプラインにはエンドポイントがあり、Azure portal で確認できます。また、公開されたパイプライン オブジェクトのプロパティとして見つけることもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rest_endpoint = published_pipeline.endpoint\n",
        "print(rest_endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "エンドポイントを使用するには、クライアント アプリケーションが HTTP 経由で REST 呼び出しを行う必要があります。この要求は認証される必要があるため、Authorization ヘッダーが必要です。これをテストするには、現在の接続から Azure ワークスペースへの Authorization ヘッダーを使用します。これは、次のコードを使用して取得できます。\n",
        "\n",
        "> **注**: 実際のアプリケーションには、認証に使用するサービス プリンシパルが必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()\n",
        "print('Authentication header ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "これで、REST インターフェイスを呼び出す準備ができました。パイプラインは非同期で実行されるため、識別子を取得します。実行中のパイプライン実験を追跡するために使用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": \"mslearn-diabetes-batch\"})\n",
        "run_id = response.json()[\"Id\"]\n",
        "run_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "実行 ID があるため、**RunDetails** ウィジェットを使用して、実行中の実験を表示できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments['mslearn-diabetes-batch'], run_id)\n",
        "\n",
        "# 実行が完了するまでブロックする\n",
        "published_pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "パイプラインの実行が完了するのを待ってから、以下のセルを実行して結果を確認します。\n",
        "\n",
        "前と同様に、結果は最初のパイプライン ステップの出力にあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# 以前の実行で残っている場合は、ローカル結果フォルダーを削除する\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
        "\n",
        "# 最初のステップの実行を取得し、その出力をダウンロードします\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='diabetes-results')\n",
        "\n",
        "# フォルダー階層を走査して、結果ファイルを見つける\n",
        "for root, dirs, files in os.walk('diabetes-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# クリーンアップ出力形式\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "\n",
        "# 最初の 20 件の結果を表示する\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "これで、毎日の患者データをバッチ処理するために使用できるパイプラインが作成されました。\n",
        "\n",
        "**詳細情報**: バッチ推論用パイプラインの使用方法の詳細については、Azure Machine Learning のドキュメントの[「バッチ予測の実行方法」](https://docs.microsoft.com/azure/machine-learning/how-to-run-batch-predictions)を参照してください。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}